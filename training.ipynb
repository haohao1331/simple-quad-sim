{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yefan/opt/miniconda3/envs/cds245/lib/python3.10/site-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Clear all imports from previous runs\n",
    "sys.modules.pop('dataset', None)\n",
    "sys.modules.pop('model', None)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from dataset import get_all_wind_directions\n",
    "from dataset import TrajectoryDataset\n",
    "from model import Phi_Net, H_Net_CrossEntropy, Model, save_model, load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]/Users/yefan/Desktop/CDS245/simple-quad-sim/dataset.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.data = pd.concat([self.data, pd.DataFrame({\n",
      "100%|██████████| 125/125 [00:00<00:00, 297.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (500000, 11) Y.shape: (500000, 3) c.shape: (500000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]/Users/yefan/Desktop/CDS245/simple-quad-sim/dataset.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.data = pd.concat([self.data, pd.DataFrame({\n",
      "100%|██████████| 125/125 [00:00<00:00, 2065.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (500000, 11) Y.shape: (500000, 3) c.shape: (500000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]/Users/yefan/Desktop/CDS245/simple-quad-sim/dataset.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.data = pd.concat([self.data, pd.DataFrame({\n",
      "100%|██████████| 125/125 [00:00<00:00, 1678.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (500000, 11) Y.shape: (500000, 3) c.shape: (500000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]/Users/yefan/Desktop/CDS245/simple-quad-sim/dataset.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.data = pd.concat([self.data, pd.DataFrame({\n",
      "100%|██████████| 125/125 [00:00<00:00, 1645.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (500000, 11) Y.shape: (500000, 3) c.shape: (500000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]/Users/yefan/Desktop/CDS245/simple-quad-sim/dataset.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.data = pd.concat([self.data, pd.DataFrame({\n",
      "100%|██████████| 125/125 [00:00<00:00, 2096.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (500000, 11) Y.shape: (500000, 3) c.shape: (500000,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('/Users/yefan/Desktop/CDS245/simple-quad-sim/data')\n",
    "amplitudes = [0, 0.5, 1, 1.5, 2]\n",
    "datasets = [TrajectoryDataset(data_dir, c) for c in amplitudes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "omegas = [0, 0.5, 1, 1.5, 2]\n",
    "wind_directions = get_all_wind_directions(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims of (x, y) are (11, 3)\n",
      "there are 5 different conditions\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "dim_x = 4 + 4 + 3\n",
    "dim_y = 3\n",
    "num_c = len(amplitudes)\n",
    "dim_a = 3\n",
    "learning_rate = 5e-4\n",
    "\n",
    "print('dims of (x, y) are', (dim_x, dim_y))\n",
    "print('there are ' + str(num_c) + ' different conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Trainloader = []\n",
    "Adaptloader = []\n",
    "for i in range(len(amplitudes)):\n",
    "    fullset = datasets[i]\n",
    "\n",
    "    l = len(fullset)\n",
    "    trainset, adaptset = random_split(fullset, [int(l*2/3), l-int(l*2/3)])\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "    adaptloader = DataLoader(adaptset, batch_size=32, shuffle=True)\n",
    "\n",
    "    Trainloader.append(trainloader)\n",
    "    Adaptloader.append(adaptloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_net = Phi_Net(dim_x, dim_a)\n",
    "h_net = H_Net_CrossEntropy(dim_a, num_c)\n",
    "criterion = nn.MSELoss()\n",
    "criterion_h = nn.CrossEntropyLoss()\n",
    "optimizer_h = optim.Adam(h_net.parameters(), lr=learning_rate)\n",
    "optimizer_phi = optim.Adam(phi_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss_f: 5.39 loss_c: 1.06\n",
      "[11] loss_f: 2.21 loss_c: 1.06\n",
      "[21] loss_f: 5.06 loss_c: 1.06\n",
      "[31] loss_f: 4.91 loss_c: 1.06\n",
      "[41] loss_f: 6.43 loss_c: 1.06\n",
      "[51] loss_f: 4.22 loss_c: 1.06\n",
      "[61] loss_f: 6.60 loss_c: 1.06\n",
      "[71] loss_f: 2.31 loss_c: 1.06\n",
      "[81] loss_f: 3.13 loss_c: 1.06\n",
      "[91] loss_f: 3.33 loss_c: 1.06\n",
      "[101] loss_f: 5.36 loss_c: 1.06\n",
      "[111] loss_f: 5.03 loss_c: 1.06\n",
      "[121] loss_f: 1.80 loss_c: 1.06\n",
      "[131] loss_f: 2.16 loss_c: 1.06\n",
      "[141] loss_f: 3.36 loss_c: 1.06\n",
      "[151] loss_f: 2.57 loss_c: 1.06\n",
      "[161] loss_f: 1.80 loss_c: 1.06\n",
      "[171] loss_f: 2.82 loss_c: 1.06\n",
      "[181] loss_f: 2.42 loss_c: 1.06\n",
      "[191] loss_f: 3.66 loss_c: 1.06\n",
      "[201] loss_f: 4.27 loss_c: 1.06\n",
      "[211] loss_f: 3.50 loss_c: 1.06\n",
      "[221] loss_f: 4.02 loss_c: 1.06\n",
      "[231] loss_f: 2.25 loss_c: 1.06\n",
      "[241] loss_f: 3.45 loss_c: 1.06\n",
      "[251] loss_f: 2.43 loss_c: 1.06\n",
      "[261] loss_f: 2.59 loss_c: 1.06\n",
      "[271] loss_f: 2.84 loss_c: 1.06\n",
      "[281] loss_f: 2.89 loss_c: 1.06\n",
      "[291] loss_f: 2.58 loss_c: 1.06\n",
      "[301] loss_f: 1.33 loss_c: 1.06\n",
      "[311] loss_f: 1.92 loss_c: 1.06\n",
      "[321] loss_f: 2.60 loss_c: 1.06\n",
      "[331] loss_f: 2.25 loss_c: 1.06\n",
      "[341] loss_f: 1.75 loss_c: 1.06\n",
      "[351] loss_f: 2.94 loss_c: 1.06\n",
      "[361] loss_f: 2.73 loss_c: 1.06\n",
      "[371] loss_f: 2.25 loss_c: 1.06\n",
      "[381] loss_f: 2.54 loss_c: 1.06\n",
      "[391] loss_f: 2.02 loss_c: 1.06\n",
      "[401] loss_f: 3.17 loss_c: 1.06\n",
      "[411] loss_f: 3.24 loss_c: 1.06\n",
      "[421] loss_f: 2.81 loss_c: 1.06\n",
      "[431] loss_f: 4.52 loss_c: 1.06\n",
      "[441] loss_f: 2.07 loss_c: 1.06\n",
      "[451] loss_f: 2.72 loss_c: 1.06\n",
      "[461] loss_f: 2.16 loss_c: 1.06\n",
      "[471] loss_f: 2.92 loss_c: 1.06\n",
      "[481] loss_f: 2.41 loss_c: 1.06\n",
      "[491] loss_f: 1.48 loss_c: 1.06\n",
      "[501] loss_f: 10.18 loss_c: 1.06\n",
      "[511] loss_f: 2.58 loss_c: 1.06\n",
      "[521] loss_f: 2.29 loss_c: 1.06\n",
      "[531] loss_f: 2.97 loss_c: 1.06\n",
      "[541] loss_f: 3.60 loss_c: 1.06\n",
      "[551] loss_f: 3.00 loss_c: 1.06\n",
      "[561] loss_f: 1.50 loss_c: 1.06\n",
      "[571] loss_f: 1.61 loss_c: 1.06\n",
      "[581] loss_f: 2.23 loss_c: 1.06\n",
      "[591] loss_f: 2.99 loss_c: 1.06\n",
      "[601] loss_f: 2.04 loss_c: 1.06\n",
      "[611] loss_f: 2.83 loss_c: 1.06\n",
      "[621] loss_f: 2.12 loss_c: 1.06\n",
      "[631] loss_f: 1.91 loss_c: 1.06\n",
      "[641] loss_f: 2.01 loss_c: 1.06\n",
      "[651] loss_f: 1.81 loss_c: 1.06\n",
      "[661] loss_f: 2.60 loss_c: 1.06\n",
      "[671] loss_f: 4.65 loss_c: 1.06\n",
      "[681] loss_f: 5.52 loss_c: 1.06\n",
      "[691] loss_f: 2.11 loss_c: 1.06\n",
      "[701] loss_f: 2.62 loss_c: 1.06\n",
      "[711] loss_f: 2.91 loss_c: 1.06\n",
      "[721] loss_f: 3.34 loss_c: 1.06\n",
      "[731] loss_f: 2.25 loss_c: 1.06\n",
      "[741] loss_f: 2.75 loss_c: 1.06\n",
      "[751] loss_f: 3.56 loss_c: 1.06\n",
      "[761] loss_f: 3.53 loss_c: 1.06\n",
      "[771] loss_f: 2.11 loss_c: 1.06\n",
      "[781] loss_f: 3.52 loss_c: 1.06\n",
      "[791] loss_f: 3.96 loss_c: 1.06\n",
      "[801] loss_f: 4.30 loss_c: 1.06\n",
      "[811] loss_f: 2.32 loss_c: 1.06\n",
      "[821] loss_f: 3.36 loss_c: 1.06\n",
      "[831] loss_f: 2.41 loss_c: 1.06\n",
      "[841] loss_f: 2.85 loss_c: 1.06\n",
      "[851] loss_f: 2.47 loss_c: 1.06\n",
      "[861] loss_f: 3.97 loss_c: 1.06\n",
      "[871] loss_f: 3.34 loss_c: 1.06\n",
      "[881] loss_f: 2.19 loss_c: 1.06\n",
      "[891] loss_f: 2.94 loss_c: 1.06\n",
      "[901] loss_f: 5.68 loss_c: 1.06\n",
      "[911] loss_f: 2.63 loss_c: 1.06\n",
      "[921] loss_f: 2.47 loss_c: 1.06\n",
      "[931] loss_f: 3.95 loss_c: 1.06\n",
      "[941] loss_f: 2.01 loss_c: 1.06\n",
      "[951] loss_f: 2.99 loss_c: 1.06\n",
      "[961] loss_f: 4.90 loss_c: 1.06\n",
      "[971] loss_f: 2.13 loss_c: 1.06\n",
      "[981] loss_f: 3.08 loss_c: 1.06\n",
      "[991] loss_f: 2.61 loss_c: 1.06\n"
     ]
    }
   ],
   "source": [
    "model_save_freq = 50\n",
    "\n",
    "Loss_f = [] # combined force prediction loss\n",
    "Loss_c = [] # combined adversarial loss\n",
    "\n",
    "alpha = 0.01\n",
    "SN = 2\n",
    "\n",
    "for epoch in range(1000):\n",
    "    arr = np.arange(len(amplitudes))\n",
    "    np.random.shuffle(arr)\n",
    "\n",
    "    # Running loss over all subdatasets\n",
    "    running_loss_f = 0.0\n",
    "    running_loss_c = 0.0\n",
    "\n",
    "    for i in arr:\n",
    "        with torch.no_grad():\n",
    "            adaptloader = Adaptloader[i]\n",
    "            kshot_data = next(iter(adaptloader))\n",
    "            trainloader = Trainloader[i]\n",
    "            data = next(iter(trainloader))\n",
    "        \n",
    "        optimizer_phi.zero_grad()\n",
    "\n",
    "        '''\n",
    "        Least-square to get $a$ from K-shot data\n",
    "        '''\n",
    "        X = kshot_data[0]\n",
    "        Y = kshot_data[1]\n",
    "        phi = phi_net(X)\n",
    "        phi_T = phi.transpose(0, 1)\n",
    "        A = torch.inverse(torch.mm(phi_T, phi))\n",
    "        a = torch.mm(torch.mm(A, phi_T), Y)\n",
    "\n",
    "        if torch.norm(a, 'fro') > 10:\n",
    "            a = a / torch.norm(a, 'fro') * 10\n",
    "        \n",
    "        '''\n",
    "        Batch training \\phi_net\n",
    "        '''\n",
    "        inputs = data[0]\n",
    "        labels = data[1]\n",
    "\n",
    "        c_labels = data[2].type(torch.long)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = torch.mm(phi_net(inputs), a)\n",
    "        loss_f = criterion(outputs, labels)\n",
    "        temp = phi_net(inputs)\n",
    "\n",
    "        loss_c = criterion_h(h_net(temp), c_labels)\n",
    "\n",
    "        loss_phi = loss_f - alpha * loss_c\n",
    "        loss_phi.backward()\n",
    "        optimizer_phi.step()\n",
    "\n",
    "        '''\n",
    "        Discriminator training\n",
    "        '''\n",
    "        if np.random.rand() <= 1.0 / 2:\n",
    "            optimizer_h.zero_grad()\n",
    "            temp = phi_net(inputs)\n",
    "            \n",
    "            loss_c = criterion_h(h_net(temp), c_labels)\n",
    "            \n",
    "            loss_h = loss_c\n",
    "            loss_h.backward()\n",
    "            optimizer_h.step()\n",
    "        \n",
    "        '''\n",
    "        Spectral normalization\n",
    "        '''\n",
    "        if SN > 0:\n",
    "            for param in phi_net.parameters():\n",
    "                M = param.detach().numpy()\n",
    "                if M.ndim > 1:\n",
    "                    s = np.linalg.norm(M, 2)\n",
    "                    if s > SN:\n",
    "                        param.data = param / s * SN\n",
    "        \n",
    "        running_loss_f += loss_f.item()\n",
    "        running_loss_c += loss_c.item()\n",
    "    \n",
    "    # Save statistics\n",
    "    Loss_f.append(running_loss_f / len(amplitudes))\n",
    "    Loss_c.append(running_loss_c / len(amplitudes))\n",
    "    if epoch % 10 == 0:\n",
    "        print('[%d] loss_f: %.2f loss_c: %.2f' % (epoch + 1, running_loss_f / len(amplitudes), running_loss_c / len(amplitudes)))\n",
    "\n",
    "    if epoch % model_save_freq == 0:\n",
    "        save_model(phi_net=phi_net, h_net=h_net, modelname=f'epoch{epoch}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds245",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
